# International Comparison: AI Security Frameworks

> **How Australia compares to global AI security approaches**

This document compares Australia's AI security landscape against major international frameworks: the European Union, United Kingdom, United States, Singapore, Canada, and Japan.

---

## Comparison Matrix

| Dimension | ğŸ‡¦ğŸ‡º Australia | ğŸ‡ªğŸ‡º EU | ğŸ‡¬ğŸ‡§ UK | ğŸ‡ºğŸ‡¸ US | ğŸ‡¸ğŸ‡¬ Singapore | ğŸ‡¨ğŸ‡¦ Canada |
|-----------|--------------|--------|--------|--------|---------------|------------|
| **Binding AI Security Law** | âŒ No | âœ… AI Act | âš ï¸ Emerging | âš ï¸ State-level | âŒ No | âš ï¸ Sector |
| **Risk Classification** | âŒ No | âœ… 4-tier | âš ï¸ Principles | âš ï¸ NIST RMF | âœ… AI Verify | âš ï¸ ADM levels |
| **Adversarial Testing** | âš ï¸ Detection only | âœ… Mandatory | âœ… AISI conducts | âš ï¸ Recommended | âœ… Moonshot | âŒ No |
| **AI Security Body** | âŒ Fragmented | âš ï¸ Distributed | âœ… AISI | âš ï¸ NIST/NSA | âœ… IMDA | âŒ No |
| **Supply Chain** | âš ï¸ Guidance | âœ… Mandatory | âš ï¸ Code | âš ï¸ NIST guidance | âš ï¸ AI Verify | âŒ No |
| **Foundation Models** | âŒ No | âœ… GPAI rules | âœ… AISI focus | âš ï¸ EO (rescinded) | âš ï¸ Emerging | âŒ No |
| **Incident Reporting** | âŒ No (AI-specific) | âœ… High-risk AI | âš ï¸ Emerging | âš ï¸ Sector | âŒ No | âŒ No |
| **Private Sector** | âŒ Voluntary | âœ… All high-risk | âš ï¸ Voluntary+ | âš ï¸ Mixed | âš ï¸ Voluntary+ | âš ï¸ Federal only |

**Legend:** âœ… Comprehensive | âš ï¸ Partial/Emerging | âŒ Gap

---

## European Union

### EU AI Act

The EU AI Act is the world's first comprehensive, legally binding AI regulation. It entered into force in August 2024 with phased implementation through 2027.

**Key AI Security Provisions:**

#### Article 15: Accuracy, Robustness and Cybersecurity

High-risk AI systems must:
- Achieve appropriate levels of accuracy, robustness, and cybersecurity
- Be resilient against attempts by unauthorised third parties to alter use, outputs or performance
- Address technical solutions for:
  - **Data poisoning**
  - **Model poisoning**
  - **Adversarial examples**
  - **Model flaws**

#### Article 53: GPAI Provider Obligations

General-purpose AI providers must:
- Provide technical documentation
- Make information available to downstream providers
- Document training data (summary)
- Comply with copyright requirements

#### Article 55: Systemic Risk GPAI

GPAI models with systemic risk must additionally:
- Perform model evaluation
- Assess and mitigate systemic risks
- **Conduct adversarial testing**
- Report serious incidents
- Ensure cybersecurity protections

### Enforcement

| Violation | Maximum Penalty |
|-----------|-----------------|
| Prohibited AI practices | â‚¬35M or 7% global turnover |
| High-risk non-compliance | â‚¬15M or 3% global turnover |
| Incorrect information | â‚¬7.5M or 1% global turnover |

### Australia vs EU

| Aspect | Australia | EU |
|--------|-----------|-----|
| Private sector binding requirements | âŒ No | âœ… Yes |
| Risk classification system | âŒ No | âœ… 4-tier |
| Adversarial testing mandate | âŒ No | âœ… Systemic risk GPAI |
| Conformity assessment | âŒ No | âœ… High-risk AI |
| Penalties for non-compliance | âŒ N/A | âœ… Up to 7% turnover |

---

## United Kingdom

### AI Security Institute (AISI)

The UK renamed its AI Safety Institute to the **AI Security Institute** in February 2025, explicitly expanding mandate to include:
- National security applications
- Cyberattacks and fraud
- Adversarial robustness
- Model security evaluation

**Key Activities:**
- Red-teaming of frontier models
- Security evaluation methodology
- Published research on adversarial attacks
- International cooperation

### UK AI Cyber Security Code of Practice

Published January 2025, provides 13 principles across 5 lifecycle phases:

**Lifecycle Phases:**
1. Secure Design
2. Secure Development
3. Secure Deployment
4. Secure Maintenance
5. Secure End-of-Life

**Security Requirements:**
- Security awareness training (regular, risk-based frequency)
- Secure coding training for AI developers
- Threat modelling including AI-specific attacks
- Supply chain security
- Incident response for AI systems

### Australia vs UK

| Aspect | Australia | UK |
|--------|-----------|-----|
| Dedicated AI Security body | âŒ AISI is Safety-focused | âœ… AISI renamed to Security |
| AI Cyber Security Code | âŒ No | âœ… 13 principles, 5 phases |
| Red-teaming capability | âŒ No | âœ… AISI conducts |
| Training requirements | âŒ No | âœ… regular security training |
| Frontier model evaluation | âš ï¸ AISI planned | âœ… Active program |

---

## United States

### Federal Landscape

The US approach is fragmented across agencies with sector-specific requirements.

**Key Frameworks:**

#### NIST AI Risk Management Framework (AI RMF)

Voluntary framework with four core functions:
1. **Govern** - Cultivate AI risk management culture
2. **Map** - Identify and categorise AI risks
3. **Measure** - Analyse and assess AI risks
4. **Manage** - Prioritise and act on risks

**Security Coverage:**
- "Secure and Resilient" is core trustworthy AI characteristic
- Addresses adversarial robustness
- Supply chain risk management
- Incident management

#### NIST Adversarial Machine Learning Report

Comprehensive taxonomy covering:
- **Evasion attacks** - Manipulating inputs to cause misclassification
- **Poisoning attacks** - Corrupting training data
- **Privacy attacks** - Extracting training data or model information
- **Availability attacks** - Denial of service against AI systems

#### NSA AI Security Center

Established to:
- Develop AI security guidance
- Coordinate with Five Eyes partners (including ACSC)
- Focus on national security AI applications

#### DHS AI Guidelines for Critical Infrastructure

14 February 2024 guidelines categorise three risk areas:
1. Attacks **using** AI
2. Attacks **targeting** AI systems
3. AI **design failures**

### Executive Order 14110 (Rescinded)

The October 2023 Executive Order established AI safety requirements including:
- Red-teaming definitions
- Reporting requirements for dual-use foundation models
- Security standards development

**Note:** EO 14110 was rescinded in January 2025. Impact on AI security requirements ongoing.

### Australia vs US

| Aspect | Australia | US |
|--------|-----------|-----|
| Risk management framework | âŒ No | âœ… NIST AI RMF |
| Adversarial ML taxonomy | âŒ No | âœ… NIST AI 100-2 |
| AI security research | Limited | âœ… >$700M annually (NSF) |
| CI AI guidelines | âš ï¸ CISC factsheet | âœ… DHS comprehensive |
| Five Eyes coordination | âœ… Via ACSC | âœ… NSA AISC |

---

## Singapore

### National AI Strategy 2.0

Singapore's updated AI strategy emphasises trusted AI with practical implementation tools.

**Key Initiatives:**

#### AI Verify

World's first AI governance testing framework:
- Self-assessment tool
- Technical testing
- Maps to international standards (ISO/IEC 42001)
- Process-based and technical checks

**Testing Areas:**
- Transparency
- Fairness
- Robustness
- Accountability
- Data governance

#### Project Moonshot

World's first open-source LLM red-teaming toolkit:
- Automated testing
- Manual adversarial testing
- Jailbreak detection
- Prompt injection testing
- Benchmark against attack libraries

**Freely available:** [github.com/aiverify-foundation/moonshot](https://github.com/aiverify-foundation/moonshot)

#### Model AI Governance Framework

Practical guidance for:
- Internal governance
- Human oversight
- Operations management
- Stakeholder communication

### Australia vs Singapore

| Aspect | Australia | Singapore |
|--------|-----------|-----------|
| AI governance tool | âŒ No | âœ… AI Verify |
| Red-teaming toolkit | âŒ No | âœ… Project Moonshot (open source) |
| Model governance framework | âš ï¸ NAIC guidance | âœ… Comprehensive framework |
| Testing/certification | âŒ No | âœ… AI Verify certification |
| International standards alignment | âš ï¸ References | âœ… Maps to ISO 42001 |

---

## Canada

### Federal Approach

#### Directive on Automated Decision-Making

Mandatory for all federal government AI:

**Impact Assessment Levels:**
| Level | Impact | Requirements |
|-------|--------|--------------|
| Level I | Little to no | Basic documentation |
| Level II | Moderate | Peer review, testing |
| Level III | High | Expert review, public notice |
| Level IV | Very High | Full review, legal compliance |

**Security Requirements:**
- Security assessments during development
- Measures to secure data and model integrity
- Prevention of tampering and unauthorized modifications
- Regular testing and monitoring

#### Algorithmic Impact Assessment Tool

Public tool for assessing AI systems:
- 106 questions
- Generates impact level
- Identifies mitigation requirements

### Australia vs Canada

| Aspect | Australia | Canada |
|--------|-----------|--------|
| Government AI mandate | âœ… DTA Policy | âœ… ADM Directive |
| Impact assessment tool | âŒ No | âœ… AIA Tool |
| Security in AI directive | âš ï¸ General reference | âœ… Explicit requirements |
| Tamper prevention | âŒ Not specified | âœ… Required |
| Public transparency | âš ï¸ DTA transparency statements | âœ… Mandatory for Level III+ |

---

## Japan

### AI Safety Institute (Japan AISI)

Established 14 February 2024, Japan's AISI focuses on:
- AI safety evaluation
- Evaluation methodology development
- International cooperation

**Published:**
- AI Safety Evaluation Perspectives v1.01
- Guidelines for AI development
- Testing methodology

### AI Security Coverage

Japan AISI evaluation perspectives include:
- Robustness against adversarial inputs
- Security of AI systems
- Privacy protection
- Misuse prevention

### Australia vs Japan

| Aspect | Australia | Japan |
|--------|-----------|-------|
| Safety Institute | âœ… AISI (planned early 2026) | âœ… AISI (operational) |
| Evaluation methodology | âŒ Not published | âœ… Published v1.01 |
| Adversarial robustness | âš ï¸ ISM control | âœ… In evaluation framework |
| International cooperation | âœ… INASI member | âœ… INASI member |

---

## International Network of AI Safety Institutes (INASI)

Australia is a member of INASI, connecting with peer institutes:

**Members:**
- ğŸ‡¬ğŸ‡§ UK AISI (AI Security Institute)
- ğŸ‡ºğŸ‡¸ US AISI
- ğŸ‡¯ğŸ‡µ Japan AISI
- ğŸ‡°ğŸ‡· Korea AISI
- ğŸ‡«ğŸ‡· France AISI
- ğŸ‡¨ğŸ‡¦ Canada AISI
- ğŸ‡¸ğŸ‡¬ Singapore (observer)
- ğŸ‡¦ğŸ‡º Australia AISI (joining)
- ğŸ‡ªğŸ‡º EU AI Office (observer)

**Focus Areas:**
- Frontier AI safety
- Evaluation methodology sharing
- Joint research
- Information sharing on risks

**Note:** Network focuses on AI Safety; AI Security coordination varies by member.

---

## Key Lessons for Australia

### From EU
- **Lesson:** Mandatory requirements drive compliance
- **Action:** Consider binding AI security requirements for high-risk AI

### From UK
- **Lesson:** Safety and Security need unified approach
- **Action:** Expand AISI mandate or rename to Security Institute

### From Singapore
- **Lesson:** Practical tools enable adoption
- **Action:** Develop Australian AI security assessment toolkit

### From US
- **Lesson:** Comprehensive taxonomy enables understanding
- **Action:** Adopt or develop Australian adversarial ML framework

### From Canada
- **Lesson:** Government can lead by example
- **Action:** Strengthen DTA policy with explicit security requirements

---

## Standards Alignment

### ISO/IEC Standards

| Standard | Description | Australia Status |
|----------|-------------|------------------|
| ISO/IEC 42001:2023 | AI Management Systems | Referenced, not mandated |
| ISO/IEC 23894:2023 | AI Risk Management | Referenced |
| ISO/IEC 27001:2022 | Information Security | ISM aligns |
| ISO/IEC 27701:2019 | Privacy Information Management | OAIC references |

### Emerging Standards

| Standard | Description | Status |
|----------|-------------|--------|
| ISO/IEC 27090 | AI Security Guidelines | In development |
| ISO/IEC 24029 | AI Robustness | In development |
| IEEE 2841 | AI Security Taxonomy | In development |

---

## Conclusion

Australia's AI security approach has strengths in technical guidance (ACSC) and government policy (ISM, PSPF), but lags international peers in:

1. **Mandatory private sector requirements** (EU leads)
2. **Unified Safety/Security approach** (UK leads)
3. **Practical assessment tools** (Singapore leads)
4. **Adversarial ML framework** (US leads)
5. **Government AI security requirements** (Canada leads)

Closing these gaps requires learning from international approaches while adapting to Australian context.

---

[â† Back to Index](../README.md) | [Gap Analysis â†’](GAPS.md) | [Knowledge Graph â†’](KNOWLEDGE-GRAPH.md)
